{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer:\n",
        "Ensemble Learning is a machine learning technique that combines multiple individual models (called base learners or weak learners) to create a stronger and more accurate predictive model.\n",
        "The key idea is that a group of weak models, when combined properly, can outperform any single strong model.\n",
        "Each model captures different aspects of the data, and combining them reduces errors due to bias, variance, or noise.\n",
        "Common ensemble methods include Bagging, Boosting, and Stacking."
      ],
      "metadata": {
        "id": "5MQL2wcuSGl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer: Meaning:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Combines predictions from multiple independent models trained in parallel on different subsets of data.\n",
        "\n",
        "Boosting: Builds models sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
        "\n",
        "Training Approach:\n",
        "\n",
        "Bagging: Models are trained in parallel and independently.\n",
        "\n",
        "Boosting: Models are trained sequentially, with each model learning from the mistakes of the previous ones.\n",
        "\n",
        "Sampling Technique:\n",
        "\n",
        "Bagging: Uses bootstrap sampling (sampling with replacement).\n",
        "\n",
        "Boosting: Uses the entire dataset, but adjusts sample weights based on errors.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Bagging: Aims to reduce variance of the model.\n",
        "\n",
        "Boosting: Aims to reduce both bias and variance.\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "Bagging: Treats all models equally and averages their outputs.\n",
        "\n",
        "Boosting: Assigns higher weights to misclassified samples to improve performance on hard cases.\n",
        "\n",
        "Model Combination:\n",
        "\n",
        "Bagging: Combines predictions by majority voting (classification) or averaging (regression).\n",
        "\n",
        "Boosting: Combines models using weighted voting or weighted sum.\n",
        "\n",
        "Base Learners:\n",
        "\n",
        "Bagging: Uses strong learners like fully grown Decision Trees.\n",
        "\n",
        "Boosting: Often uses weak learners (shallow trees).\n",
        "\n",
        "Risk of Overfitting:\n",
        "\n",
        "Bagging: Less prone to overfitting.\n",
        "\n",
        "Boosting: Can overfit if not properly tuned (e.g., high learning rate, too many estimators).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Bagging: Random Forest.\n",
        "\n",
        "Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
        "\n",
        "Performance:\n",
        "\n",
        "Bagging: Improves stability and reduces variance.\n",
        "\n",
        "Boosting: Increases accuracy by reducing both bias and variance."
      ],
      "metadata": {
        "id": "iAwgWLbGSTPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:\n",
        "Bootstrap sampling is a statistical technique where multiple random samples are drawn with replacement from the training data.\n",
        "Each base learner (e.g., Decision Tree) in Bagging or Random Forest is trained on a different bootstrap sample.\n",
        "\n",
        "Role in Bagging:\n",
        "\n",
        "Ensures diversity among base learners.\n",
        "\n",
        "Reduces variance by averaging predictions from models trained on different subsets.\n",
        "\n",
        "Allows estimation of model performance using Out-of-Bag (OOB) samples."
      ],
      "metadata": {
        "id": "Di3n7JH3Tk3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "OOB samples are data points not included in a bootstrap sample used to train a specific model in an ensemble.\n",
        "In Random Forests, about one-third of the data remains “out-of-bag” for each tree.\n",
        "\n",
        "OOB Score:\n",
        "\n",
        "Each tree predicts the labels for its OOB samples.\n",
        "\n",
        "The OOB score is the average accuracy (or error) over all these predictions.\n",
        "\n",
        "It provides an unbiased internal estimate of model performance without needing a separate validation set."
      ],
      "metadata": {
        "id": "q1-nzpEUTshh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "A single Decision Tree measures feature importance by how much each feature decreases impurity (e.g., Gini or Entropy) in splits.\n",
        "\n",
        "A Random Forest averages feature importance across all trees, giving more stable and robust importance rankings.\n",
        "\n",
        "Random Forests reduce bias from individual trees and are less sensitive to random fluctuations in data."
      ],
      "metadata": {
        "id": "D6hK8vJ8Txkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Python program – Random Forest feature importance on Breast Cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top_features = feature_importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dh2pKIBT9KG",
        "outputId": "084ea3ff-a7fd-4759-cb3d-bc98f4d4ba59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Python program – Bagging Classifier vs Single Decision Tree on Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                        n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsE9lG_vUHht",
        "outputId": "efd4ba46-d972-4260-f3cd-2ef81bf14174"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Python program – Random Forest with GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model and parameter grid\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMDKaZ4ZUfz2",
        "outputId": "853f6cb3-62f6-4cfa-e203-bfe8de415a17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Python program – Compare Bagging and Random Forest Regressors\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train models\n",
        "bag = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compare\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4wtnk2SU2u-",
        "outputId": "7596a1e3-c7e6-44e7-cac2-2919e218a47c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2572988359842641\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Case Study – Predicting Loan Default using Ensemble Techniques\n",
        "Step 1: Choose between Bagging or Boosting\n",
        "\n",
        "Start with Boosting (e.g., XGBoost) if the dataset has complex, non-linear patterns and you need high accuracy.\n",
        "\n",
        "Use Bagging (Random Forest) when variance is high and you want robust, interpretable models.\n",
        "\n",
        "Step 2: Handle Overfitting\n",
        "\n",
        "Use cross-validation and early stopping (for Boosting).\n",
        "\n",
        "Limit max depth of trees and tune learning rate.\n",
        "\n",
        "Use OOB score for internal validation in Bagging.\n",
        "\n",
        "Step 3: Select Base Models\n",
        "\n",
        "Decision Trees as base learners (fast and flexible).\n",
        "\n",
        "Ensemble multiple weak learners to reduce variance and bias.\n",
        "\n",
        "Step 4: Evaluate Performance\n",
        "\n",
        "Use k-fold cross-validation.\n",
        "\n",
        "Metrics: accuracy, F1-score, ROC-AUC.\n",
        "\n",
        "Compare ensemble vs single models.\n",
        "\n",
        "Step 5: Justify Ensemble Learning Benefits\n",
        "\n",
        "Increases prediction stability and reduces risk of false loan approvals/denials.\n",
        "\n",
        "Captures complex relationships in financial data.\n",
        "\n",
        "Enhances decision-making for credit risk assessment."
      ],
      "metadata": {
        "id": "mcU-OGmFVBhZ"
      }
    }
  ]
}